{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c92671a",
   "metadata": {},
   "source": [
    "# LSR: Latent Structure Refinement\n",
    "\n",
    "Paper: https://aclanthology.org/2020.acl-main.141.pdf\n",
    "\n",
    "Git: https://github.com/aisingapore/sgnlp/tree/main"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6135f0",
   "metadata": {},
   "source": [
    "## LSR Training\n",
    "\n",
    "Currently not working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21f4e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import re\n",
    "import spacy\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import os\n",
    "import traceback\n",
    "\n",
    "# Install required packages if not already installed\n",
    "# !pip install sgnlp spacy\n",
    "# !python -m spacy download en_core_web_sm\n",
    "\n",
    "from sgnlp.sgnlp.models.lsr import LsrModel, LsrConfig, LsrPreprocessor, LsrPostprocessor\n",
    "\n",
    "# --- Device selection ---\n",
    "device_to_use = 0 if torch.cuda.is_available() else -1\n",
    "device = torch.device(f'cuda:{device_to_use}' if device_to_use >= 0 else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# --- Load NER model for entity recognition ---\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# --- Load LSR model and preprocessor ---\n",
    "print(\"Loading LSR model files...\")\n",
    "\n",
    "# load the files from /content/ directory\n",
    "rel2id_path = '/content/rel2id.json'\n",
    "word2id_path = '/content/word2id.json'\n",
    "ner2id_path = '/content/ner2id.json'\n",
    "rel_info_path = '/content/rel_info.json'\n",
    "\n",
    "# Set prediction threshold\n",
    "PRED_THRESHOLD = 0.3\n",
    "\n",
    "# Initialize preprocessor and postprocessor\n",
    "try:\n",
    "    preprocessor = LsrPreprocessor(\n",
    "        rel2id_path=rel2id_path, \n",
    "        word2id_path=word2id_path, \n",
    "        ner2id_path=ner2id_path\n",
    "    )\n",
    "\n",
    "    postprocessor = LsrPostprocessor.from_file_paths(\n",
    "        rel2id_path=rel2id_path, \n",
    "        rel_info_path=rel_info_path, \n",
    "        pred_threshold=PRED_THRESHOLD\n",
    "    )\n",
    "\n",
    "    # Load model\n",
    "    config_path = \"/content/config.json\"\n",
    "    model_path = \"/content/pytorch_model.bin\"\n",
    "\n",
    "    config = LsrConfig.from_json_file(config_path)\n",
    "    model = LsrModel(config)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    print(\"LSR model loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading LSR model: {e}\")\n",
    "    traceback.print_exc()\n",
    "\n",
    "# Map spaCy entity types to LSR expected types\n",
    "def map_entity_type(ent_type):\n",
    "    \"\"\"Map spaCy entity types to LSR entity types\"\"\"\n",
    "    # This mapping might need adjustment based on ner2id.json content\n",
    "    type_mapping = {\n",
    "        'PERSON': 'PER',\n",
    "        'ORG': 'ORG',\n",
    "        'GPE': 'LOC',\n",
    "        'LOC': 'LOC',\n",
    "        'NORP': 'MISC',\n",
    "        'CARDINAL': 'NUM',\n",
    "        'DATE': 'TIME',\n",
    "        'MONEY': 'NUM',\n",
    "        'TIME': 'TIME',\n",
    "        'PERCENT': 'NUM',\n",
    "        'PRODUCT': 'MISC',\n",
    "        'EVENT': 'MISC',\n",
    "        'FAC': 'LOC',\n",
    "        'WORK_OF_ART': 'MISC',\n",
    "        'LAW': 'MISC',\n",
    "        'LANGUAGE': 'MISC',\n",
    "        'QUANTITY': 'NUM'\n",
    "    }\n",
    "    # Default to 'MISC' if not in mapping\n",
    "    return type_mapping.get(ent_type, 'MISC')\n",
    "\n",
    "def get_sentences(text):\n",
    "    \"\"\"Split text into sentences\"\"\"\n",
    "    doc = nlp(text)\n",
    "    return [sent.text.strip() for sent in doc.sents]\n",
    "\n",
    "def extract_entities(text):\n",
    "    \"\"\"Extract entities from text using spaCy with type mapping\"\"\"\n",
    "    doc = nlp(text)\n",
    "    entities = []\n",
    "    entity_to_idx = {}\n",
    "    idx = 0\n",
    "    \n",
    "    # Extract entities by sentence\n",
    "    sentences = list(doc.sents)\n",
    "    \n",
    "    for sent_idx, sent in enumerate(sentences):\n",
    "        for ent in sent.ents:\n",
    "            # Skip entities that are too short\n",
    "            if len(ent.text.strip()) < 2:\n",
    "                continue\n",
    "                \n",
    "            # Map entity type to LSR compatible type\n",
    "            mapped_type = map_entity_type(ent.label_)\n",
    "            \n",
    "            # Create entity object compatible with LSR format\n",
    "            entity = {\n",
    "                \"name\": ent.text,\n",
    "                \"pos\": [ent.start - sent.start, ent.end - sent.start],\n",
    "                \"sent_id\": sent_idx,\n",
    "                \"type\": mapped_type  # Use mapped entity type\n",
    "            }\n",
    "            \n",
    "            # Group mentions of the same entity (simple exact match)\n",
    "            if ent.text not in entity_to_idx:\n",
    "                entity_to_idx[ent.text] = idx\n",
    "                entities.append([])\n",
    "                idx += 1\n",
    "            \n",
    "            entities[entity_to_idx[ent.text]].append(entity)\n",
    "    \n",
    "    return entities\n",
    "\n",
    "def prepare_lsr_input(text):\n",
    "    \"\"\"Prepare input in the format required by LSR model\"\"\"\n",
    "    # Get sentences from text\n",
    "    text = text[:5000]  # Limit text length to prevent issues\n",
    "    sentences = get_sentences(text)\n",
    "    \n",
    "    if not sentences:\n",
    "        return None\n",
    "    \n",
    "    # Extract entities with mapped types\n",
    "    vertex_set = extract_entities(text)\n",
    "    \n",
    "    # Skip if no entities found\n",
    "    if not vertex_set:\n",
    "        return None\n",
    "    \n",
    "    # Format sentences as tokens\n",
    "    tokenized_sents = [[token for token in sent.split()] for sent in sentences]\n",
    "    \n",
    "    # LSR requires this specific format\n",
    "    instance = {\n",
    "        \"vertexSet\": vertex_set,\n",
    "        \"labels\": [],  # No pre-defined labels when extracting\n",
    "        \"sents\": tokenized_sents\n",
    "    }\n",
    "    \n",
    "    return instance\n",
    "\n",
    "def extract_triplets_lsr(text):\n",
    "    \"\"\"Extract triplets using the LSR model with improved error handling\"\"\"\n",
    "    triplets = []\n",
    "    \n",
    "    try:\n",
    "        # Prepare input for the LSR model\n",
    "        instance = prepare_lsr_input(text)\n",
    "        \n",
    "        # Skip processing if no valid input\n",
    "        if not instance:\n",
    "            return triplets\n",
    "        \n",
    "        # Process through LSR pipeline\n",
    "        model_inputs = preprocessor([instance])\n",
    "        \n",
    "        # Skip if preprocessing failed\n",
    "        if not model_inputs:\n",
    "            return triplets\n",
    "        \n",
    "        # Move inputs to the right device\n",
    "        for key in model_inputs.keys():\n",
    "            if isinstance(model_inputs[key], torch.Tensor):\n",
    "                model_inputs[key] = model_inputs[key].to(device)\n",
    "        \n",
    "        # Get predictions\n",
    "        with torch.no_grad():\n",
    "            predictions = model(model_inputs)\n",
    "        \n",
    "        # Skip if no predictions\n",
    "        if not predictions or \"logits\" not in predictions:\n",
    "            return triplets\n",
    "        \n",
    "        # Post-process predictions\n",
    "        processed_preds = postprocessor(\n",
    "            instances=[instance],\n",
    "            pred_logits=predictions[\"logits\"].cpu().numpy()\n",
    "        )\n",
    "        \n",
    "        # Convert predictions to triplets format\n",
    "        for pred in processed_preds[0]:\n",
    "            head_idx = pred.get('h_idx')\n",
    "            tail_idx = pred.get('t_idx')\n",
    "            \n",
    "            # Skip if invalid indices\n",
    "            if (head_idx is None or tail_idx is None or \n",
    "                head_idx >= len(instance[\"vertexSet\"]) or \n",
    "                tail_idx >= len(instance[\"vertexSet\"]) or\n",
    "                not instance[\"vertexSet\"][head_idx] or \n",
    "                not instance[\"vertexSet\"][tail_idx]):\n",
    "                continue\n",
    "            \n",
    "            # Extract triplet information\n",
    "            head_entity = instance[\"vertexSet\"][head_idx][0][\"name\"]\n",
    "            tail_entity = instance[\"vertexSet\"][tail_idx][0][\"name\"]\n",
    "            relation = pred.get('pred_rel', 'unknown')\n",
    "            head_type = instance[\"vertexSet\"][head_idx][0].get(\"type\", \"unknown\")\n",
    "            tail_type = instance[\"vertexSet\"][tail_idx][0].get(\"type\", \"unknown\")\n",
    "            \n",
    "            # Add triplet to results\n",
    "            triplets.append({\n",
    "                'head': head_entity,\n",
    "                'head_type': head_type,\n",
    "                'type': relation,\n",
    "                'tail': tail_entity,\n",
    "                'tail_type': tail_type\n",
    "            })\n",
    "            \n",
    "    except Exception as e:\n",
    "        # Print detailed error information\n",
    "        print(f\"Error extracting triplets with LSR: {e}\")\n",
    "        traceback.print_exc()\n",
    "        \n",
    "    return triplets\n",
    "\n",
    "# --- Main processing function ---\n",
    "def process_data(input_csv, output_csv, debug=False):\n",
    "    \"\"\"Process data with better error handling and debugging options\"\"\"\n",
    "    print(f\"Loading data from {input_csv}...\")\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(input_csv)\n",
    "        all_triplets = []\n",
    "        \n",
    "        for index, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing documents with LSR\"):\n",
    "            try:\n",
    "                document = row['ID']\n",
    "                text = row['Text']\n",
    "                \n",
    "                if not isinstance(text, str) or len(text.strip()) == 0:\n",
    "                    if debug:\n",
    "                        print(f\"Skipping document {document}: Invalid text\")\n",
    "                    continue\n",
    "                \n",
    "                # Process text in manageable chunks\n",
    "                max_chunk_length = 500  # Reduced chunk size for better stability\n",
    "                chunks = [text[i:i+max_chunk_length] for i in range(0, len(text), max_chunk_length)]\n",
    "                \n",
    "                document_triplets = []\n",
    "                \n",
    "                for chunk_idx, chunk in enumerate(chunks):\n",
    "                    if debug and chunk_idx > 0:\n",
    "                        print(f\"Processing chunk {chunk_idx+1}/{len(chunks)} for document {document}\")\n",
    "                    \n",
    "                    # Extract triplets using LSR\n",
    "                    chunk_triplets = extract_triplets_lsr(chunk)\n",
    "                    document_triplets.extend(chunk_triplets)\n",
    "                \n",
    "                # Add to results\n",
    "                for i, triplet in enumerate(document_triplets, 1):\n",
    "                    all_triplets.append({\n",
    "                        \"DOCUMENT\": document,\n",
    "                        \"SUBLABEL\": i,\n",
    "                        \"MODEL\": \"lsr\",\n",
    "                        \"HEAD\": triplet.get('head', ''),\n",
    "                        \"RELATION\": triplet.get('type', ''),\n",
    "                        \"TAIL\": triplet.get('tail', ''),\n",
    "                        \"HEAD_TYPE\": triplet.get('head_type', ''),\n",
    "                        \"TAIL_TYPE\": triplet.get('tail_type', '')\n",
    "                    })\n",
    "                    \n",
    "                if debug and document_triplets:\n",
    "                    print(f\"Found {len(document_triplets)} triplets in document {document}\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing document at index {index}: {e}\")\n",
    "                if debug:\n",
    "                    traceback.print_exc()\n",
    "        \n",
    "        # Create DataFrame and save results\n",
    "        result_df = pd.DataFrame(all_triplets)\n",
    "        result_df.to_csv(output_csv, index=False)\n",
    "        print(f\"Processing complete. Found {len(all_triplets)} triplets. Saved to {output_csv}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in process_data: {e}\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "# Run the processor\n",
    "if __name__ == \"__main__\":\n",
    "    # Set debug=True to get more detailed output\n",
    "    process_data('JuanRana_split.csv', 'lsr_triplets_output.csv', debug=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
